// TODO MILAN
const browser = require('./browser');
//const logger = require('../utilities/logger').child({ component: 'puppeteer_scraper.js' });

/**
 * Function to get the page data from a URL with Puppeteer.
 * NOTE: There is no rate limiting on this function!!
 * @param {String} url - The URL to get the page data from.
 * @param {Object} browser - The Puppeteer browser instance to use.
 * @returns {String} The page data.
 */
const getPageDataWithPuppeteer = async (url) => {
  // start the browser page
  await browser.launch();
  
  const page = await browser.getInstance().newPage();
  
  try {
    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.5563 Safari/537.36');

    await page.goto(url, {
      // wait until there are no more network requests for 500ms, this seems to be needed for some websites with doms generated by javascript
      waitUntil: 'networkidle0',
    });

    const hrefs = await page.$$eval('a[href*="zssk_mimoriadne/status/"]', links => {
      const regex = /^https:\/\/twitter.com\/zssk_mimoriadne\/status\/\d+$/;
      return links.filter(link => regex.test(link.href)).map(link => link.href);
    });
    
    const timestamps = await page.$$eval('time[datetime]', (dates) => dates.map((date) => date.dateTime));

    const tweets = await page.$$eval('article div[lang]', (tweets) => tweets.map((tweet) => tweet.textContent));

    const combined = hrefs.map((href, index) => {
      return {
        href,
        timestamp: timestamps[index],
        tweet: tweets[index]
      };
    });
    return combined

  } catch (error) {
    console.log(error);
    //logger.error('Something went wrong while trying to get page data with Puppeteer! Closing the page! Err: ${error}');
  } finally {
    // close the page
    if (page) await page.close();
  }
};

module.exports = {
  getPageDataWithPuppeteer,
};